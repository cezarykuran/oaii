% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api_chat.R
\name{chat_request}
\alias{chat_request}
\title{API chat: send completions request}
\usage{
chat_request(
  api_key,
  messages,
  model = "gpt-3.5-turbo",
  temperature = 0.7,
  n = 1,
  max_tokens = 50,
  presence_penalty = 0,
  frequency_penalty = 0
)
}
\arguments{
\item{api_key}{string, API key}

\item{messages}{data.frame, data.frame with messages comprising the conversation so far}

\item{model}{string, ID of the model to use.
See the \href{https://platform.openai.com/docs/models/model-endpoint-compatibility}{model endpoint compatibility table}
for details on which models work with the Chat API.}

\item{temperature}{double, what sampling temperature to use, between 0 and 2.
Higher values like 0.8 will make the output more random,
while lower values like 0.2 will make it more focused and deterministic.}

\item{n}{integer, how many chat completion choices
to generate for each input message.}

\item{max_tokens}{integer, the maximum number of tokens to generate
in the chat completion}

\item{presence_penalty}{double, number between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.}

\item{frequency_penalty}{double, number between -2.0 and 2.0.
Positive values penalize new tokens based on their existing frequency
in the text so far, decreasing the model's likelihood to repeat the same line
verbatim.}
}
\value{
content of the httr response object or SimpleError enhanced with
two additional fields: status_code (response$status_code) and message_long
(built on response content)
}
\description{
\url{https://platform.openai.com/docs/api-reference/completions}
}
\examples{
\dontrun{
  res_content <- chat_request(
    api_key = "my-secret-api-key-string",
    messages = chat_messages("hi")
  )
  if (!is_error(res_content)) {
    answer <- chat_fetch_messages(res_content)
    conversation <- chat_merge_messages(question, answer)
    print(conversation)
  }
}

}
